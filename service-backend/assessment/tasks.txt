# service-backend/assessment/tasks.py
"""
Celery tasks for the assessment app.
This file handles background processing related to assessments,
primarily preparing and sending completed results to the AI service.
"""

# Import Celery instance (Will be configured later in core/celery.py)
try:
    from celery import shared_task
    # from django.core.exceptions import ObjectDoesNotExist
    from django.conf import settings
    from django.db.models import Prefetch
    import logging

    # Import models and services
    from .models import UserAssessmentAttempt # To get completed attempts
    # Import the service function that prepares the data
    from .services import prepare_aggregated_package_data_for_ai
    # Import the AI integration service function
    from ai_integration.services import send_to_deepseek_api # Import the AI service function

    logger = logging.getLogger(__name__)

except ImportError as e:
    # Handle potential import errors during setup/early stages if needed
    # This is more of a safeguard; typically imports should work.
    print(f"Warning: Celery or other imports failed in assessment/tasks.py: {e}")
    shared_task = lambda *args, **kwargs: lambda fn: fn # Fallback if Celery not available


# --- Placeholder for Score Calculation Service Call ---
# Although the plan is to calculate scores immediately upon submission
# in the service layer/view, a task stub can exist here if needed for
# heavy lifting or batch processing in the future.
# @shared_task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3, 'countdown': 60})
# def calculate_individual_assessment_score(self, attempt_id):
#     """
#     Celery task to calculate the score for a single completed UserAssessmentAttempt.
#     This task is typically triggered immediately after an attempt is submitted.
#     NOTE: Current plan is for this calculation to happen in the service layer/view,
#           not as a background task. This is a placeholder/skeleton.
#     """
#     try:
#         attempt = UserAssessmentAttempt.objects.get(id=attempt_id)
#     except UserAssessmentAttempt.DoesNotExist:
#         logger.error(f"UserAssessmentAttempt with id {attempt_id} does not exist for score calculation.")
#         return f"Failed: Attempt {attempt_id} not found"

#     if not attempt.is_completed:
#         logger.warning(f"Attempt {attempt_id} is not completed. Skipping score calculation.")
#         return f"Skipped: Attempt {attempt_id} is not completed"

#     if not attempt.raw_results_json:
#         logger.warning(f"Attempt {attempt_id} has no raw results data for score calculation.")
#         return f"Skipped: No raw data for score calculation in Attempt {attempt_id}"

#     try:
#         # Call the service function to perform the calculation
#         # This function should read attempt.raw_results_json and populate attempt.processed_results_json
#         # calculate_scores_service(attempt) # Example service function call
#         # attempt.refresh_from_db() # Reload to get updated processed_results_json if saved by service

#         logger.info(f"Score calculation completed for Attempt {attempt_id}.")
#         return f"Success: Score calculated for Attempt {attempt_id}"

#     except Exception as exc:
#         logger.error(f"Failed to calculate score for Attempt {attempt_id}: {exc}", exc_info=True)
#         raise self.retry(exc=exc) # Re-raise to trigger retry based on autoretry_for


# --- Primary Task: Send Aggregated Package Results to AI ---
@shared_task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3, 'countdown': 60})
def send_to_ai(self, user_id, package_id):
    """
    Celery task to aggregate completed assessment results for a user within a specific package
    and send them to the AI integration service.

    This task is triggered manually by the user (or frontend) after ensuring
    all required assessments in the package are completed.

    Args:
        user_id (int): The ID of the user whose results are being sent.
        package_id (int): The ID of the TestPackage whose results are being aggregated and sent.
    """
    # Import User model late to avoid potential circular import issues
    # Especially if User model customization is involved
    from django.contrib.auth import get_user_model
    from .models import TestPackage, Assessment # Import Assessment model
    User = get_user_model()

    try:
        user = User.objects.get(id=user_id)
        package = TestPackage.objects.get(id=package_id)
    except User.DoesNotExist:
        logger.error(f"User with id {user_id} does not exist for sending results to AI.")
        # Don't retry for permanent errors like user not found
        return f"Failed: User {user_id} not found"
    except TestPackage.DoesNotExist:
        logger.error(f"TestPackage with id {package_id} does not exist for sending results to AI.")
        # Don't retry for permanent errors like package not found
        return f"Failed: Package {package_id} not found"

    try:
        logger.info(f"Starting send_to_ai task for User {user_id}, Package {package_id}")

        # --- Core Logic: Use the Service Layer to Prepare Data ---
        # The service function handles the complex logic of aggregation.
        # It returns the structured data ready for the AI service or None on failure.
        aggregated_ai_input_data = prepare_aggregated_package_data_for_ai(user, package)

        if not aggregated_ai_input_
            error_msg = f"Failed to prepare aggregated data for User {user_id}, Package {package_id} in service layer."
            logger.error(error_msg)
            # Don't retry if the service layer indicates a fundamental prep failure
            return f"Failed: {error_msg}"

        # --- Validation: Ensure Essential Data is Present ---
        # While the service layer prepares the data, a final check here is good practice.
        # Check if there's any assessment data to send.
        assessments_data = aggregated_ai_input_data.get("assessments_data", [])
        if not assessments_
             warning_msg = f"No completed assessment data found for User {user_id}, Package {package_id}. Nothing to send to AI."
             logger.warning(warning_msg)
             # Don't retry for this condition
             return f"Skipped: {warning_msg}"

        # Optional: Further validate that each assessment has 'results'
        # If an assessment was completed but calculation failed, processed_results_json might be empty/None.
        # Decide if you want to send partial data or skip if any assessment is missing results.
        # For now, let's proceed with the data as prepared by the service layer,
        # assuming it handles missing individual results appropriately or logs them.

        # --- Send the Aggregated Data to the AI Integration Service ---
        # Call the service function in the ai_integration app.
        # This function should handle the actual HTTP request/response with the DeepSeek API.
        logger.info(f"Sending aggregated data to AI service for User {user_id}, Package {package_id}")
        # The ai_integration service function should return a status or the AI response
        ai_response_status_or_data = send_to_deepseek_api(aggregated_ai_input_data)

        # --- Handle AI Service Response ---
        # The structure of ai_response_status_or_data depends on the implementation
        # of send_to_deepseek_api. It could be a status string, a dictionary with status/data,
        # or it could raise an exception on failure (which would be caught by autoretry_for).

        # Example handling if send_to_deepseek_api returns a dict with a 'status' key:
        # if isinstance(ai_response_status_or_data, dict):
        #     status = ai_response_status_or_data.get('status', 'unknown')
        #     if status == 'success':
        #         logger.info(f"AI service responded successfully for User {user_id}, Package {package_id}.")
        #         # The ai_integration app's task (process_ai_response) should ideally handle
        #         # the AI's final response and save recommendations.
        #         # You could potentially trigger that task here if tightly coupled:
        #         # from ai_integration.tasks import process_ai_response
        #         # process_ai_response.delay(user_id, ai_response_status_or_data.get('data', {}))
        #         return f"Success: Data sent to AI. AI Status: {status}"
        #     else:
        #         error_detail = ai_response_status_or_data.get('message', 'Unknown error from AI service')
        #         logger.error(f"AI service reported failure for User {user_id}, Package {package_id}: {error_detail}")
        #         # Depending on the nature of the error, you might want to retry or not.
        #         # For now, assuming send_to_deepseek_api handles retries for transient AI issues.
        #         return f"Failed: AI Service Error: {error_detail}"
        # else:
        #     # If it's not a dict, assume it's a simple status string or success indicator from the service call
        logger.info(f"AI service call completed for User {user_id}, Package {package_id}. Response: {ai_response_status_or_data}")

        # --- Trigger Processing of AI Response (if needed) ---
        # The ai_integration app should have its own task to handle the AI's *final* response.
        # This task (`send_to_ai`) is responsible for *sending* the data.
        # If the `send_to_deepseek_api` function directly triggers the AI and waits for a response,
        # or if it's fire-and-forget, the subsequent processing is handled by ai_integration.
        # Alternatively, if tight coupling is acceptable, you could call the ai_integration task here:
        # from ai_integration.tasks import process_ai_response
        # process_ai_response.delay(user_id, ai_response_status_or_data) # Call without .delay() if calling from another task

        logger.info(f"Successfully initiated sending data to AI for User {user_id}, Package {package_id}.")
        # Return a success message or relevant identifiers
        return f"Success: Data sent to AI for User {user_id}, Package {package_id}"
        # If returning the AI response data, ensure it's serializable and consider size.
        # return {
        #     "status": "success",
        #     "message": f"Data sent to AI for User {user_id}, Package {package_id}",
        #     # "ai_response": ai_response_status_or_data # Be cautious about returning large AI responses
        # }


    except Exception as exc:
        logger.error(
            f"Failed to send data to AI for User {user_id}, Package {package_id}: {exc}",
            exc_info=True # Logs the full traceback
        )
        # Re-raise the exception to trigger retry based on the `autoretry_for` decorator
        # This is useful for transient network issues or temporary AI service unavailability.
        raise self.retry(exc=exc)
